## 信息学
### 信息熵：  
**一条信息的大小和他的不确定性相关，不确定性越大，信息量越大。  
对于一个随机事件X，其发生的概率为P(X),那么P(X)越大，信息量l越小  
且l(X)在P(X)的取值为（0，1）的区间内值不能为负数，最实用的形式就是：  
l(X)=-logP(X)  
而熵的定义是接收的每条关于变量X的消息中包含的信息的平均量，也就是X的所有信息量的期望：  
H(X) = $\sum P(X) * -logP(X)$ = $-\sum P(X) logP(X)$  
表示X的信息熵，表示变量X的不确定性 ** 
具体参考 [一问读懂信息熵，交叉熵，相对熵](https://zhuanlan.zhihu.com/p/593362066 "信息熵，交叉熵，相对熵，极大似然估计")。
