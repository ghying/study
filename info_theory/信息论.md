## 信息学
### 信息熵：  
**一条信息的大小和他的不确定性相关，不确定性越大，信息量越大。  
对于一个随机事件X，其发生的概率为P(X),那么P(X)越大，信息量l越小  
且l(X)在P(X)的取值为（0，1）的区间内值不能为负数，最实用的形式就是：  
l(X)=-logP(X)  
而熵的定义是接收的每条消息中包含的信息的平均量，也就是所有信息量的期望：  
H(X) = $\sum a_k b_k$**
